{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alone-royal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from langchain import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42548615",
   "metadata": {},
   "source": [
    "*Summarize the whole thing here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "perceived-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_name(docs):\n",
    "    \"\"\"\n",
    "    Create document.metadata['law'] to direct filename only\n",
    "    use regex to isolate filename (w/o pdf)\n",
    "    \"\"\"\n",
    "    #splits filename by /'s\n",
    "    split_path = re.split(r'\\/', docs.metadata['source'])\n",
    "    #takes the last item from the split (the item name) and gets rid of the \".pdf\" extension\n",
    "    docs.metadata['law'] = re.split(r'\\.',split_path[-1])[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-mailing",
   "metadata": {},
   "source": [
    "### Takes the folder and splits it into the separate documents. Then runs the above source_name function on all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = PyPDFDirectoryLoader(\"/Users/gv658da/Documents/tax_folder\")\n",
    "texts = documents.load_and_split()\n",
    "\n",
    "for text in texts:\n",
    "    source_name(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-franchise",
   "metadata": {},
   "source": [
    "### Creates a vector store of OpenAI embeddings from the documents, and sets up the ability to retrieve those documents from unstructured queries.\n",
    "\n",
    "\n",
    "*Why FAISS instead of something else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "color-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-information",
   "metadata": {},
   "source": [
    "### Setting up chatGPT to be the large language model used in analysis. Temperature of 0 means the responses from the model with be deterministic (no randomness)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "communist-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-causing",
   "metadata": {},
   "source": [
    "### Sets up the LLM to find the vector embeddings from the vector store that best match the query.\n",
    "\n",
    "*More detail about QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exotic-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQAWithSourcesChain.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-aggregate",
   "metadata": {},
   "source": [
    "### Takes the vectors retrieved by the chain and displays them in a user-friendly readable summary with source documents referenced below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regional-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatty(query):\n",
    "    chatbot = dict(chain({\"question\": query}, return_only_outputs=True))\n",
    "    return chatbot['answer'] + f\"\\n\" + \"Here's where I found this information: \" + chatbot['sources']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-screw",
   "metadata": {},
   "source": [
    "### Creates a convenient interface for the \"chatty\" function using Gradio. Provides an interface where a user can enter a query on the left-hand side, and chatty's response will appear on the right-hand side. demo.launch() opens the interface in browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "positive-credit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=chatty,\n",
    "    inputs=[\"text\"],\n",
    "    outputs=[\"text\"],\n",
    ")\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
